{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc6da3e-d5a9-4e10-9a56-9877584b619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e4cd54-8835-43d0-8bee-e46890b65124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340191ff-1530-4417-a199-6cee805f3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\bart_env_gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9f953-37c3-401c-883c-f4cb5329057a",
   "metadata": {},
   "source": [
    "### Se carga la gpu pero finalmente no se puede utilizar ya que da errores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8125fcf4-f303-4c43-9787-2cd9867a1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d17f6-3a36-4231-bc23-2f05ddd76c6b",
   "metadata": {},
   "source": [
    "### Tras haber configurado la gpu y importado ciertos paquetes clave, se lee el csv con las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda04ef2-fcb6-48ee-9d18-74f35a418d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape  Musicians gro...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2s desire to be number one  U2, who have won ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart  The film ada...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oceans Twelve raids box office  Oceans Twelve,...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data         labels\n",
       "0  Musicians to tackle US red tape  Musicians gro...  entertainment\n",
       "1  U2s desire to be number one  U2, who have won ...  entertainment\n",
       "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n",
       "3  Snicket tops US box office chart  The film ada...  entertainment\n",
       "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Tras haber configurado la gpu y importado ciertos paquetes clave, se lee el csv con las noticias\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ruta = 'bbc_data.csv'\n",
    "# Leer un archivo CSV (si ya lo tienes subido)\n",
    "df = pd.read_csv(ruta)\n",
    "\n",
    "# Ver las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd120273-9a9b-4c6c-a9c1-810e6e753b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7641f-0ed4-4832-bedf-fe71c39c98fe",
   "metadata": {},
   "source": [
    "### Se utiliza el modelo distilbart-cnn-12-6 ya entrenado para resumir noticias de nuestros conjunto de datos y asÃ­ tener datos con los que hacer fine tuning a un modelo que se adecÃºe tanto a nuestra noticias como a los resÃºmenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b3c176-b12a-4147-8e1f-4a4f77530c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Usando CPU (procesando 150 textos para mayor velocidad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "ðŸ”„ Generando resÃºmenes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [10:51<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Â¡ResÃºmenes generados y guardados en 'pseudo_summary_dataset_150.csv'!\n"
     ]
    }
   ],
   "source": [
    "### Se utiliza el modelo distilbart-cnn-12-6 ya entrenado para resumir noticias de nuestros conjunto de datos y asÃ­ tener datos con los que hacer fine tuning a un modelo que se adecÃºe tanto a nuestra noticias como a los resÃºmenes.\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Cargar el dataset original y quedarnos solo con las primeras 150 noticias\n",
    "df = pd.read_csv(\"pseudo_summary_dataset.csv\").head(150)\n",
    "\n",
    "# 2. Forzar uso de CPU (la GPU no tiene suficiente memoria)\n",
    "device = -1\n",
    "print(f\"âœ… Usando CPU (procesando 150 textos para mayor velocidad)\")\n",
    "\n",
    "# 3. Crear pipeline de resumen con modelo ligero\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 4. Generar resÃºmenes\n",
    "resumenes = []\n",
    "for texto in tqdm(df[\"data\"], desc=\"ðŸ”„ Generando resÃºmenes\"):\n",
    "    try:\n",
    "        texto = texto[:1024]  # truncamos si el texto es muy largo\n",
    "        resumen = summarizer(texto, max_length=60, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        resumen = f\"ERROR: {str(e)}\"\n",
    "    resumenes.append(resumen)\n",
    "\n",
    "# 5. Guardar dataset reducido con los resÃºmenes generados\n",
    "df[\"summary\"] = resumenes\n",
    "df.to_csv(\"pseudo_summary_dataset_150.csv\", index=False)\n",
    "print(\"âœ… Â¡ResÃºmenes generados y guardados en 'pseudo_summary_dataset_150.csv'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa89ae-04eb-49e2-b446-dbd9dd0e1ceb",
   "metadata": {},
   "source": [
    "### Se comprueba que no hay valores errÃ³neos que puedan dar problemas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ded6be-c0b8-4975-a468-134a1380521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data       0\n",
      "summary    0\n",
      "dtype: int64\n",
      "data       object\n",
      "summary    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Se comprueba que no hay valores errÃ³neos que puedan dar problemas en el entrenamiento\n",
    "\n",
    "from datasets import Dataset\n",
    "print(df[['data', 'summary']].isnull().sum())  # â† no debe haber NAs\n",
    "print(df.dtypes)  # â† ambas deben ser string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f26346-44f9-4e83-b5a9-85044ec77a59",
   "metadata": {},
   "source": [
    "### Se utiliza el conjunto de datos con noticias y sus respectivos resÃºmenes para hacer fine tuning al mismo modelo distilbart-cnn-12-6 y de esta manera que sea mas personalizado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8855a1-3c04-4311-934e-3ebdd41476b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Usando dispositivo: cpu\n",
      "\n",
      "ðŸ” Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [03:19<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PÃ©rdida media epoch 1: 1.3927\n",
      "\n",
      "ðŸ” Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [03:09<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PÃ©rdida media epoch 2: 0.4797\n",
      "\n",
      "ðŸ” Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [03:11<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PÃ©rdida media epoch 3: 0.3404\n",
      "\n",
      "ðŸ” Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [03:06<00:00,  2.49s/it]\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\bart_env_gpu\\lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PÃ©rdida media epoch 4: 0.2945\n",
      "ðŸŽ‰ Â¡Modelo personalizado con capas congeladas guardado en 'distilbart_custom_model_frozen'!\n"
     ]
    }
   ],
   "source": [
    "### Se utiliza el conjunto de datos con noticias y sus respectivos resÃºmenes para hacer fine tuning al mismo modelo distilbart-cnn-12-6 y de esta manera que sea mas personalizado. \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "df = pd.read_csv(\"pseudo_summary_dataset_150.csv\")  # columnas: 'data' y 'summary'\n",
    "\n",
    "# 2. Tokenizer y modelo\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.train()\n",
    "\n",
    "# 3. Forzar modo CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"ðŸ“ Usando dispositivo: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# âœ… Congelar todas las capas excepto la Ãºltima del decodificador\n",
    "for name, param in model.model.named_parameters():\n",
    "    if \"decoder.layers\" in name:\n",
    "        layer_num = int(name.split(\"decoder.layers.\")[1].split(\".\")[0])\n",
    "        if layer_num < len(model.model.decoder.layers) - 1:\n",
    "            param.requires_grad = False  # congelar todas excepto la Ãºltima\n",
    "        else:\n",
    "            param.requires_grad = True   # descongelar solo la Ãºltima\n",
    "    else:\n",
    "        param.requires_grad = False  # congelar todo lo demÃ¡s (encoder, embeddings, etc.)\n",
    "\n",
    "\n",
    "# 5. Dataset personalizado\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_len=512, max_target_len=64):\n",
    "        self.inputs = texts\n",
    "        self.targets = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = str(self.inputs[idx])\n",
    "        target_text = str(self.targets[idx])\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, max_length=self.max_input_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, max_length=self.max_target_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# 6. DataLoader\n",
    "dataset = SummaryDataset(df[\"data\"], df[\"summary\"], tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 7. Optimizador\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 8. Entrenamiento\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nðŸ” Epoch {epoch+1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Entrenando\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"âœ… PÃ©rdida media epoch {epoch+1}: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# 9. Guardar modelo fine-tuned\n",
    "model.save_pretrained(\"distilbart_custom_model_frozen\")\n",
    "tokenizer.save_pretrained(\"distilbart_custom_model_frozen\")\n",
    "print(\"ðŸŽ‰ Â¡Modelo personalizado con capas congeladas guardado en 'distilbart_custom_model_frozen'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef3397-84f2-4b54-bcc9-5446f4a5df3b",
   "metadata": {},
   "source": [
    "### Se prueban diferentes noticias del conjunto de datos. Resume bien pero hay que tener en cuenta que eran noticias que han participado en el entrenamiento y hace completamente el mismo resumen, lo cual dice que ha aprendido exactamente como se hacen los resumenes de nuestro conjunto de datos (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3c5b191-e257-41a8-aafb-d469fa9574fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\bart_env_gpu\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo cargado correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\bart_env_gpu\\lib\\site-packages\\transformers\\generation\\utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Texto original (idx 46):\n",
      "Grammys honour soul star Charles  The memory of soul legend Ray Charles dominated the music worlds leading music ceremony on Sunday as he was given eight posthumous Grammy Awards.  Charles, who died in 2004, got honours including record and album of the year, while Alicia Keys and actor Jamie Foxx performed a musical tribute to him. R&B star Keys won four awards herself at the Grammy ceremony in Los Angeles. U2, Usher, Norah Jones and Kanye West got three each. West led the race going into the ceremony with 10 nominations.  Charles last album, Genius Loves Company, a collection of duets that has sold more than two million copies, was named album of the year and best pop vocal album. His song Here We Go Again with Norah Jones won record of the year and best pop vocal collaboration, while Heaven Help Us All with Gladys Knight picked up best gospel performance. Jones said: \"Im glad hes getting recognised, because of who he is and how much I love him.\" Actor Jamie Foxx - who is nominated for an Oscar for playing Charles in the hit movie Ray - dedicated a rendition of Georgia on My Mind to \"old friends\". Keys, looking to replicate her Grammys success of 2002, when she won five, picked up best R&B song for You Dont Know My Name and best R&B album for The Diary of Alicia Keys. She also shared the award for best R&B vocal performance by a duo or group with Usher for My Boo.  Ushers other victories were for best contemporary R&B album for Confessions and best rap/sung collaboration for Yeah!, featuring Lil Jon and Ludacris. Kanye West dominated the rap categories, winning best rap song for Jesus Walks and best rap album for The College Dropout. But in one of the nights biggest shocks, he lost out in the battle to be named best new artist to pop rock act Maroon 5. Vertigo by rock giants U2 won three trophies - best rock song, best short video and best rock vocal performance by a duo or group. One of the other main awards, song of the year, went to US singer-songwriter John Mayer for Daughters. Mayer also won best male pop vocal performance. Britney Spears picked up her first ever Grammy for her song Toxic, which was named best dance recording.  Rod Stewart also won the first Grammy of his career, getting the best traditional pop album award for Stardust... The Great American Songbook: Volume III. In 2003, Stewart said he was \"astounded\" he had never won a Grammy - but \"they tend not to give it to the British unless youre Sting\". There were few other high-profile British victors this year. Annie Lennox, metal group Motorhead and dance act Basement Jaxx all took home trophies. But Elvis Costello, who had four nominations, and Joss Stone and Franz Ferdinand, who were both up for three awards, got nothing.  Beach Boys veteran Brian Wilson was another first-time winner - for best rock instrumental performance. \"It represents triumph and achievement in music that I feel that I deserved, and Im really glad I won,\" he said. A live recording of composer John Adams 11 September tribute, On the Transmigration of Souls, performed by the New York Philharmonic, won three classical prizes. And former US President Bill Clinton picked up the second Grammy of his career, winning the spoken word award for the audio version of his autobiography My Life.\n",
      "\n",
      "ðŸ“‹ Resumen real:\n",
      "Ray Charles given eight posthumous Grammy Awards at Grammy Awards in Los Angeles . Charles' last album, Genius Loves Company, won album of the year and best pop vocal album . R&B star Alicia Keys and actor Jamie Foxx performed a musical tribute to him . U2\n",
      "\n",
      "ðŸ“Œ Resumen generado:\n",
      "Ray Charles given eight posthumous Grammy Awards at Grammy Awards in Los Angeles . Charles' last album, Genius Loves Company, won album of the year and best pop vocal album . R&B star Alicia Keys and actor Jamie Foxx performed a musical tribute to him . U2\n"
     ]
    }
   ],
   "source": [
    "### Se prueban diferentes noticias del conjunto de datos. Resume bien pero hay que tener en cuenta que eran noticias que han participado en el entrenamiento y hace completamente el mismo resumen, lo cual dice que ha aprendido exactamente como se hacen los resumenes de nuestro conjunto de datos (overfitting)\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Ruta local al modelo y tokenizer en formato POSIX (muy importante evitar backslashes o errores de path)\n",
    "model_path = \"C:/Users/Usuario/AnalisisDatosNoEstructurados/ProyectoAdne_2/distilbart_custom_model_frozen\"\n",
    "\n",
    "# 2. Cargar tokenizer y modelo fine-tuned desde la carpeta local\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 3. Dispositivo (si tu GPU no da, se usarÃ¡ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Modelo cargado correctamente\")\n",
    "\n",
    "# 4. Cargar dataset reducido\n",
    "df = pd.read_csv(\"pseudo_summary_dataset_150.csv\")\n",
    "\n",
    "# 5. Seleccionar una noticia cualquiera del dataset\n",
    "idx = 46  # Cambia este Ã­ndice para ver otras noticias\n",
    "texto = df.loc[idx, \"data\"]\n",
    "resumen_real = df.loc[idx, \"summary\"]\n",
    "\n",
    "# 6. Tokenizar y generar resumen\n",
    "inputs = tokenizer(\n",
    "    texto,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=20,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "resumen_generado = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 7. Mostrar resultados\n",
    "print(f\"\\nðŸ“° Texto original (idx {idx}):\\n{texto.strip()}\")\n",
    "print(f\"\\nðŸ“‹ Resumen real:\\n{resumen_real.strip()}\")\n",
    "print(f\"\\nðŸ“Œ Resumen generado:\\n{resumen_generado.strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9a5dd-ebd2-42a6-a304-ae7e4eb2726d",
   "metadata": {},
   "source": [
    "### Se utilizan noticias generadas sintÃ©ticamente para ver que tal resume y lo hace correctamente. Todo parece indicar que el modelo es lo suficientemente robusto debido a que solo se ha entrenado la Ãºltima capa, para resumir correctamente cualquier tipo de noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44a5d371-e355-4b88-aa20-f85bd745aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Fake news:\n",
      " In a surprising turn of events, Galician scientists have discovered a new subatomic particle \n",
      "that could completely reshape our understanding of the universe. The discovery, made in an underground laboratory in Lugo, \n",
      "has been described as \"revolutionary\" by international experts, although further experiments are still being conducted \n",
      "to confirm its properties. The research team, led by Dr. Carmela PiÃ±eiro, claims the particle interacts with dark matter \n",
      "in ways never seen before. The government has already announced plans to financially support the development of new experiments \n",
      "to explore this phenomenon.\n",
      "\n",
      "ðŸ“Œ Generated summary:\n",
      " Galician scientists have discovered a new subatomic particle that could completely reshape our understanding of the universe . The discovery, made in an underground laboratory in Lugo, has been described as \"revolutionary\" by international experts . The government has already announced plans to financially support the development of\n"
     ]
    }
   ],
   "source": [
    "### Se utilizan noticias generadas sintÃ©ticamente para ver que tal resume y lo hace correctamente. Todo parece indicar que el modelo es lo suficientemente robusto debido a que solo se ha entrenado la Ãºltima capa, para resumir correctamente cualquier tipo de noticia\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Ruta al modelo fine-tuned local\n",
    "model_path = \"C:/Users/Usuario/AnalisisDatosNoEstructurados/ProyectoAdne_2/distilbart_custom_model_frozen\"\n",
    "\n",
    "# Cargar tokenizer y modelo\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Seleccionar dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Noticia inventada en inglÃ©s\n",
    "fake_news = \"\"\"\n",
    "In a surprising turn of events, Galician scientists have discovered a new subatomic particle \n",
    "that could completely reshape our understanding of the universe. The discovery, made in an underground laboratory in Lugo, \n",
    "has been described as \"revolutionary\" by international experts, although further experiments are still being conducted \n",
    "to confirm its properties. The research team, led by Dr. Carmela PiÃ±eiro, claims the particle interacts with dark matter \n",
    "in ways never seen before. The government has already announced plans to financially support the development of new experiments \n",
    "to explore this phenomenon.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar\n",
    "inputs = tokenizer(\n",
    "    fake_news,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "# Generar resumen\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=20,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decodificar\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nðŸ“° Fake news:\\n\", fake_news.strip())\n",
    "print(\"\\nðŸ“Œ Generated summary:\\n\", summary.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d14a3ef-106d-409f-ae6e-f34ae0ccabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Fake news:\n",
      " The International Space Agency has announced the successful deployment of the first solar-powered station on the moon. \n",
      "The facility, built in collaboration with European and Asian partners, is expected to provide continuous energy \n",
      "to future lunar missions. Scientists believe this marks a major milestone in sustainable space exploration \n",
      "and opens the door for permanent human presence on the moon by the 2030s.\n",
      "\n",
      "ðŸ“Œ Generated summary:\n",
      " International Space Agency has announced the successful deployment of the first solar-powered station on the moon . The facility is expected to provide continuous energy to future lunar missions . Scientists believe this marks a major milestone in sustainable space exploration .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Ruta al modelo fine-tuned local\n",
    "model_path = \"C:/Users/Usuario/AnalisisDatosNoEstructurados/ProyectoAdne_2/distilbart_custom_model_frozen\"\n",
    "\n",
    "# Cargar tokenizer y modelo\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Seleccionar dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "new_fake_news = \"\"\"\n",
    "The International Space Agency has announced the successful deployment of the first solar-powered station on the moon. \n",
    "The facility, built in collaboration with European and Asian partners, is expected to provide continuous energy \n",
    "to future lunar missions. Scientists believe this marks a major milestone in sustainable space exploration \n",
    "and opens the door for permanent human presence on the moon by the 2030s.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar\n",
    "inputs = tokenizer(\n",
    "    new_fake_news,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "# Generar resumen\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=20,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decodificar\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nðŸ“° Fake news:\\n\", new_fake_news.strip())\n",
    "print(\"\\nðŸ“Œ Generated summary:\\n\", summary.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab7814-51f2-4e04-88f0-cb037e83dc86",
   "metadata": {},
   "source": [
    "### Se prueba con una noticia mÃ¡s larga para probar si su rendimiento no se ve afectado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "156f3137-1bc8-48ce-85bc-c802d8e56495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Fake news:\n",
      " In a remarkable turn of events, an international coalition of environmental scientists and engineers has unveiled a groundbreaking technology \n",
      "capable of capturing and transforming atmospheric CO2 into clean energy at a previously unimaginable scale. \n",
      "The system, which was developed in secret over the past decade, uses a series of advanced quantum reactors that \n",
      "break down carbon dioxide molecules and restructure them into hydrogen-based fuels, emitting only water vapor in the process.\n",
      "\n",
      "The first pilot plant, built near the coast of Norway, has already started operating at 40% efficiency and is expected \n",
      "to scale up to full capacity by the end of the year. This announcement has sent ripples through global markets, with \n",
      "oil prices plummeting and green tech stocks surging. Leaders around the world have expressed both excitement and caution, \n",
      "highlighting the need for transparent regulation and fair access to what could be one of the most disruptive innovations of the century.\n",
      "\n",
      "Nobel Prize nominee Dr. Lina Matsuda, one of the lead architects of the project, stated that the team's goal is to \n",
      "â€œreset the planetâ€™s carbon balance within a generation.â€ Meanwhile, critics warn that geopolitical tensions might arise \n",
      "if the technology is monopolized or weaponized. Nevertheless, hopes are high that this discovery will usher in a new era \n",
      "of sustainable progress.\n",
      "\n",
      "ðŸ“Œ Generated summary:\n",
      " An international coalition of environmental scientists and engineers has unveiled a groundbreaking technology capable of capturing and transforming atmospheric CO2 into clean energy . The system was developed in secret over the past decade, using a series of advanced quantum reactors that break down carbon dioxide molecules and restructure them into hydrogen-based\n"
     ]
    }
   ],
   "source": [
    "### Se prueba con una noticia mÃ¡s larga para probar si su rendimiento no se ve afectado\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Ruta al modelo fine-tuned local\n",
    "model_path = \"C:/Users/Usuario/AnalisisDatosNoEstructurados/ProyectoAdne_2/distilbart_custom_model_frozen\"\n",
    "\n",
    "# Cargar tokenizer y modelo\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Seleccionar dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "long_fake_news = \"\"\"\n",
    "In a remarkable turn of events, an international coalition of environmental scientists and engineers has unveiled a groundbreaking technology \n",
    "capable of capturing and transforming atmospheric CO2 into clean energy at a previously unimaginable scale. \n",
    "The system, which was developed in secret over the past decade, uses a series of advanced quantum reactors that \n",
    "break down carbon dioxide molecules and restructure them into hydrogen-based fuels, emitting only water vapor in the process.\n",
    "\n",
    "The first pilot plant, built near the coast of Norway, has already started operating at 40% efficiency and is expected \n",
    "to scale up to full capacity by the end of the year. This announcement has sent ripples through global markets, with \n",
    "oil prices plummeting and green tech stocks surging. Leaders around the world have expressed both excitement and caution, \n",
    "highlighting the need for transparent regulation and fair access to what could be one of the most disruptive innovations of the century.\n",
    "\n",
    "Nobel Prize nominee Dr. Lina Matsuda, one of the lead architects of the project, stated that the team's goal is to \n",
    "â€œreset the planetâ€™s carbon balance within a generation.â€ Meanwhile, critics warn that geopolitical tensions might arise \n",
    "if the technology is monopolized or weaponized. Nevertheless, hopes are high that this discovery will usher in a new era \n",
    "of sustainable progress.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar\n",
    "inputs = tokenizer(\n",
    "    long_fake_news,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "# Generar resumen\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=60,\n",
    "    min_length=20,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decodificar\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nðŸ“° Fake news:\\n\", long_fake_news.strip())\n",
    "print(\"\\nðŸ“Œ Generated summary:\\n\", summary.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2704da5-1f05-489c-8bc1-14c6a3b42e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
